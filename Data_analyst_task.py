# -*- coding: utf-8 -*-
"""Copy of snappfood.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eLQcCX_YsLFVDCyaE9oH3I3oTK8jXDHF

# **Importing Libraries**
"""

#importing the necessary libraries for process data 
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime as dt

"""# **Data Pre Processing**"""

df = pd.read_csv('/content/Data analyst Task.csv')

df.head()

df.describe()
df.info()

df['discount_type'] = df['discount_type'].fillna(0)

df.info()

df.describe()

"""## **Change dataset date format to datetime**"""

#change date format 
df['created_at']=pd.to_datetime(df['created_at'])

# a custom function to extract each date argument

def get_month(argument):
  return dt.datetime(argument.year,argument.month,1)

#find duplication

df.duplicated(subset=['user_id'])

df.info()

#extract date
df['InvoiceMonth']=df['created_at'].apply(get_month)
df.head(10)

"""## **Cohort Analysis Pre Processing**"""

#starting by group dataset with InvoiceMonth and user id
df['Cohort Month'] =  df.groupby('user_id')['InvoiceMonth'].transform('min')
df.tail(10)

# a custom function to do subtraction process on datetimes data type
def get_subtraction_difference(df, column):
    day = df[column].dt.day
    month = df[column].dt.month
    year = df[column].dt.year
    return day, month, year 

_,Invoice_month,Invoice_year =  get_subtraction_difference(df,'InvoiceMonth')
_,Cohort_month,Cohort_year =  get_subtraction_difference(df,'Cohort Month')

Cohort_year

#create a cohort index 
year_diff = Invoice_year -Cohort_year
month_diff = Invoice_month - Cohort_month
df['CohortIndex'] = year_diff*12+month_diff+1
df.head()

cohort_data = df.groupby(['Cohort Month','CohortIndex'])['user_id'].apply(pd.Series.nunique).reset_index()
cohort_data.head()

"""# **Pivot Table**"""

cohort_table = cohort_data.pivot(index='Cohort Month', columns=['CohortIndex'],values='user_id')
cohort_table

"""## **Plot pivot table data**"""

plt.figure(figsize=(21,10))
cohort_table.index = cohort_table.index.strftime('%B %Y')
sns.heatmap(cohort_table,annot=True,cmap='Blues')

new_cohort_table = cohort_table.divide(cohort_table.iloc[:,0],axis=0)
new_cohort_table

plt.figure(figsize=(21,10))
sns.heatmap(new_cohort_table,annot=True,fmt='.0%',cmap='Blues')

"""## **Data point analysis to use regressor**"""

data_points=np.array(new_cohort_table.values.tolist())

data_points[np.isnan(data_points)]=0

data_points[3:8,1]
cohertindex=[1,2,3,4,5]

plt.plot(cohertindex,data_points[3:8,1])

"""## **Polynomial and linear regression**

### **linear regression**
"""

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit((np.array(cohertindex)).reshape(-1, 1),data_points[3:8,1]);

#y_predict = regressor.predict(np.array([6]).reshape(1, -1))
y_predict = regressor.predict(np.array(cohertindex).reshape(-1, 1))
regressor.intercept_
y_predict

plt.plot(cohertindex,data_points[3:8,1]*100)
plt.plot(np.array(cohertindex),y_predict*100,linestyle = '--')
#plt.scatter(6,y_predict*100)
plt.xlabel='precentaqe'
plt.show()

np.array(cohertindex)

"""# **Train and apply Polynomial Regression**"""

from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 4)
X_poly = poly_reg.fit_transform((np.array(cohertindex)).reshape(-1, 1))
lin_reg_2 = LinearRegression()
lin_reg_2.fit(X_poly, data_points[3:8,1])

X_grid = np.arange(min((np.array(cohertindex)).reshape(-1,1)), max((np.array(cohertindex)).reshape(-1,1)), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter((np.array(cohertindex)).reshape(-1,1), data_points[3:8,1], color = 'red')
plt.plot(X_grid, lin_reg_2.predict(poly_reg.fit_transform(X_grid)), color = 'blue')
plt.title(' (Polynomial Regression)')
plt.xlabel='months'
plt.ylabel='retention percentage'
plt.show()

lin_reg_2.predict(poly_reg.fit_transform([[6]]))*100

data_points[3:8,1].mean()*100

discount = pd.read_csv('/content/Data analyst Task.csv')

#discount['total_profit']=(discount['discount_cost']+discount['basket'])

discount['discount_type'] = discount['discount_type'].fillna(0)
print(discount.loc[df['discount_type'] == 2, 'basket'].mean())
print(discount.loc[df['discount_type'] == 1, 'basket'].mean())
print(discount.loc[df['discount_type'] == 0, 'basket'].mean())

plt.figure(figsize=(20,10))
sns.pairplot(discount)

discount

"""# **Pre Processing for RFM Analysis**"""

discount = pd.read_csv('/content/Data analyst Task.csv')

discount['discount_type'] = discount['discount_type'].fillna(0)

discount['created_at']=pd.to_datetime(discount['created_at'])

#Calculate the first and last orders for each user
discount['first order'] =  discount.groupby('user_id')['created_at'].transform('min')
discount['last order'] =  discount.groupby('user_id')['created_at'].transform('max')

discount.head(100)

discount=discount.sort_values(by=['user_id','created_at'])

discount.head(100)

cleaned_data=discount.drop_duplicates(subset=['user_id']) 
#drop people who dont have discount or dont have discount in exactly first order

cleaned_data

#filter users which use discount

filtered_discount_user=cleaned_data[cleaned_data["discount_type"] != 0]
filtered_discount_user

"""# **RFM Analysis**

**RFM** (Recency, Frequency, Monetary) analysis is a customer segmentation technique that uses past purchase behavior to divide customers into groups. RFM helps divide customers into various categories or clusters to identify customers who are more likely to respond to promotions and also for future personalization services.



*   RECENCY (R): Days since last purchase
*   FREQUENCY (F): Total number of purchases
*   MONETARY VALUE (M): Total money this customer spent.

We will create those 3 customer attributes for each customer.


**Customer Type 1 (Discount_type 1 and 2 )**

# **Recency**

**To calculate recency, we need to choose a date point from which we evaluate how many days ago was the customer's last purchase.**
"""

recency_df = filtered_discount_user.groupby(by='user_id', as_index=False)['created_at'].max()
recency_df.columns = ['user_id','LastPurshaceDate']
recency_df=recency_df.sort_values(by=['user_id'])
recency_df.dtypes

#get current datetime (Timezone)
today=dt.date.today().strftime('%Y-%m-%d')
date_object = dt.datetime.strptime(today, '%Y-%m-%d').date()
print(type(date_object))
print(date_object)
now = dt.date(2022,9,14)

recency_df['Recency'] = recency_df['LastPurshaceDate'].apply(lambda x: (pd.to_datetime(now) - x).days)

recency_df

"""# **Frequency**

Frequency helps us to know how many times a customer purchased from us. To do that we need to check **how many invoices are registered by the same customer**.
"""

discount = pd.read_csv('/content/Data analyst Task.csv')
discount['discount_type'] = discount['discount_type'].fillna(0)
discount['created_at']=pd.to_datetime(discount['created_at'])

frequency_df=discount[discount['user_id'].isin(recency_df['user_id'].tolist())]
frequency_df

frequency_df = frequency_df.groupby(by=['user_id'], as_index=False)['created_at'].count()
frequency_df.columns = ['user_id','Frequency']
frequency_df

"""# **Monetary**

Monetary attribute described as **How much money did the customer spent over time**
we will create a new column total cost to have the total price per invoice.
"""

discount = pd.read_csv('/content/Data analyst Task.csv')
discount['discount_type'] = discount['discount_type'].fillna(0)
discount['created_at']=pd.to_datetime(discount['created_at'])
discount=discount[discount['user_id'].isin(recency_df['user_id'].tolist())]

monetary_df = discount.groupby(by='user_id',as_index=False).agg({'basket': 'sum'})
monetary_df.columns = ['user_id','Monetary']
monetary_df.head()

"""# **Create RFM Table**"""

#merge recency dataframe with frequency dataframe
recency_df.drop('LastPurshaceDate',axis=1,inplace=True)
temp_df = recency_df.merge(frequency_df,on='user_id')
temp_df.head()

#merge with monetary dataframe to get a table with the 3 columns
rfm_df = temp_df.merge(monetary_df,on='user_id')
#use CustomerID as index
rfm_df.set_index('user_id',inplace=True)
#check the head
rfm_df.head()

"""## **Customer segments with RFM Model**
### **We do the segmentation in 2 different ways**

The simplest way to create customers segments from RFM Model is to use Quartiles. We assign a score from 1 to 4 to Recency, Frequency and Monetary. Four is the best/highest value, and one is the lowest/worst value. A final RFM score is calculated simply by combining individual RFM score numbers.
"""

quantiles = rfm_df.quantile(q=[0.25,0.5,0.75])
quantiles.to_dict()

# Arguments (x = value, p = recency, monetary_value, frequency, d = quartiles dict)
def RScore(x,p,d):
    if x <= d[p][0.25]:
        return 4
    elif x <= d[p][0.50]:
        return 3
    elif x <= d[p][0.75]: 
        return 2
    else:
        return 1
# Arguments (x = value, p = recency, monetary_value, frequency, k = quartiles dict)
def FMScore(x,p,d):
    if x <= d[p][0.25]:
        return 1
    elif x <= d[p][0.50]:
        return 2
    elif x <= d[p][0.75]: 
        return 3
    else:
        return 4

rfm_segmentation = rfm_df
rfm_segmentation['R_Quartile'] = rfm_segmentation['Recency'].apply(RScore, args=('Recency',quantiles,))
rfm_segmentation['F_Quartile'] = rfm_segmentation['Frequency'].apply(FMScore, args=('Frequency',quantiles,))
rfm_segmentation['M_Quartile'] = rfm_segmentation['Monetary'].apply(FMScore, args=('Monetary',quantiles,))
rfm_segmentation.head()

rfm_segmentation['RFMScore'] = rfm_segmentation.R_Quartile.map(str) \
                            + rfm_segmentation.F_Quartile.map(str) \
                            + rfm_segmentation.M_Quartile.map(str)
rfm_segmentation.head()

rfm_segmentation[rfm_segmentation['RFMScore']=='444'].sort_values('Monetary', ascending=False).head(10)

print("Best Customers: ",len(rfm_segmentation[rfm_segmentation['RFMScore']=='444']))
print('Loyal Customers: ',len(rfm_segmentation[rfm_segmentation['F_Quartile']==4]))
print("Big Spenders: ",len(rfm_segmentation[rfm_segmentation['M_Quartile']==4]))
print('Almost Lost: ', len(rfm_segmentation[rfm_segmentation['RFMScore']=='244']))
print('Lost Customers: ',len(rfm_segmentation[rfm_segmentation['RFMScore']=='144']))
print('Lost Cheap Customers: ',len(rfm_segmentation[rfm_segmentation['RFMScore']=='111']))

"""# **Second Procedure**"""

rfm_df = temp_df.merge(monetary_df,on='user_id')
df.set_index(['user_id'])
rfm_df['R_rank'] = rfm_df['Recency'].rank(ascending=False)
rfm_df['F_rank'] = rfm_df['Frequency'].rank(ascending=True)
rfm_df['M_rank'] = rfm_df['Monetary'].rank(ascending=True)
 
# normalizing the rank of the customers
rfm_df['R_rank_norm'] = (rfm_df['R_rank']/rfm_df['R_rank'].max())*100
rfm_df['F_rank_norm'] = (rfm_df['F_rank']/rfm_df['F_rank'].max())*100
rfm_df['M_rank_norm'] = (rfm_df['F_rank']/rfm_df['M_rank'].max())*100
 
rfm_df.drop(columns=['R_rank', 'F_rank', 'M_rank'], inplace=True)
 
rfm_df.head()

rfm_df['RFM_Score'] = 0.15*rfm_df['R_rank_norm']+0.28 * \
    rfm_df['F_rank_norm']+0.57*rfm_df['M_rank_norm']
rfm_df['RFM_Score'] *= 0.05
rfm_df = rfm_df.round(2)
rfm_df.head(7)

rfm_df["Customer_segment"] = np.where(rfm_df['RFM_Score'] >
                                      4.5, "Top Customers",
                                      (np.where(
                                        rfm_df['RFM_Score'] > 4,
                                        "High value Customer",
                                        (np.where(
    rfm_df['RFM_Score'] > 3,
                             "Medium Value Customer",
                             np.where(rfm_df['RFM_Score'] > 1.6,
                            'Low Value Customers', 'Lost Customers'))))))
rfm_df[['user_id', 'RFM_Score', 'Customer_segment']].head(20)

explode = (0, 0.1, 0, 0,0)  
fig1, ax1 = plt.subplots()
ax1.pie(rfm_df.Customer_segment.value_counts(), explode=explode, labels=rfm_df.Customer_segment.value_counts().index, autopct='%1.1f%%',
        shadow=True, startangle=90,colors=['tab:blue','tab:orange','tab:green','tab:red','tab:purple','tab:brown','tab:pink','tab:gray','tab:olive','tab:cyan'])

ax1.axis('equal')  
plt.tight_layout()
plt.show()

"""# **RFM Analysis**
## **Customer Type 2 (non_discount)**
"""

discount = pd.read_csv('/content/Data analyst Task.csv')
discount['discount_type'] = discount['discount_type'].fillna(0)
discount['created_at']=pd.to_datetime(discount['created_at'])

discount['first order'] =  discount.groupby('user_id')['created_at'].transform('min')
discount['last order'] =  discount.groupby('user_id')['created_at'].transform('max')
discount=discount.sort_values(by=['user_id','created_at'])
discount

temp_df=discount[~discount['user_id'].isin(recency_df['user_id'].tolist())]
#drop people who have discount to filter type 2 users

temp_df

#drop duplications
cleaned_data=temp_df.drop_duplicates(subset=['user_id']) 
cleaned_data

"""# **Recency**"""

recency_df_wd = cleaned_data.groupby(by='user_id', as_index=False)['created_at'].max()
recency_df_wd.columns = ['user_id','LastPurshaceDate']
recency_df_wd=recency_df_wd.sort_values(by=['user_id'])
recency_df_wd.dtypes

today=dt.date.today().strftime('%Y-%m-%d')
date_object = dt.datetime.strptime(today, '%Y-%m-%d').date()
print(type(date_object))
print(date_object)
now = dt.date(2022,9,14)

recency_df_wd['Recency'] = recency_df_wd['LastPurshaceDate'].apply(lambda x: (pd.to_datetime(now) - x).days)
recency_df_wd.drop('LastPurshaceDate',axis=1,inplace=True)

recency_df_wd

temp_df
frequency_df = temp_df.groupby(by=['user_id'], as_index=False)['created_at'].count()
frequency_df.columns = ['user_id','Frequency']
frequency_df

discount = pd.read_csv('/content/Data analyst Task.csv')
discount['discount_type'] = discount['discount_type'].fillna(0)
discount['created_at']=pd.to_datetime(discount['created_at'])
discount=discount[~discount['user_id'].isin(recency_df['user_id'].tolist())]

monetary_df = discount.groupby(by='user_id',as_index=False).agg({'basket': 'sum'})
monetary_df.columns = ['user_id','Monetary']
monetary_df

"""## **Create second table**"""

#merge recency dataframe with frequency dataframe

temp_df = recency_df_wd.merge(frequency_df,on='user_id')

#merge with monetary dataframe to get a table with the 3 columns
rfm_df = temp_df.merge(monetary_df,on='user_id')
#check the head
rfm_df = temp_df.merge(monetary_df,on='user_id')
df.set_index(['user_id'])
rfm_df['R_rank'] = rfm_df['Recency'].rank(ascending=False)
rfm_df['F_rank'] = rfm_df['Frequency'].rank(ascending=True)
rfm_df['M_rank'] = rfm_df['Monetary'].rank(ascending=True)
 
# normalizing the rank of the customers
rfm_df['R_rank_norm'] = (rfm_df['R_rank']/rfm_df['R_rank'].max())*100
rfm_df['F_rank_norm'] = (rfm_df['F_rank']/rfm_df['F_rank'].max())*100
rfm_df['M_rank_norm'] = (rfm_df['F_rank']/rfm_df['M_rank'].max())*100
 
rfm_df.drop(columns=['R_rank', 'F_rank', 'M_rank'], inplace=True)
 
rfm_df.head()

rfm_df['RFM_Score'] = 0.15*rfm_df['R_rank_norm']+0.28 * \
    rfm_df['F_rank_norm']+0.57*rfm_df['M_rank_norm']
rfm_df['RFM_Score'] *= 0.05
rfm_df = rfm_df.round(2)

rfm_df["Customer_segment"] = np.where(rfm_df['RFM_Score'] >
                                      4.5, "Top Customers",
                                      (np.where(
                                        rfm_df['RFM_Score'] > 4,
                                        "High value Customer",
                                        (np.where(
    rfm_df['RFM_Score'] > 3,
                             "Medium Value Customer",
                             np.where(rfm_df['RFM_Score'] > 1.6,
                            'Low Value Customers', 'Lost Customers'))))))
rfm_df[['user_id', 'RFM_Score', 'Customer_segment']].head(20)

explode = (0, 0.1, 0, 0,0)  
fig1, ax1 = plt.subplots()
ax1.pie(rfm_df.Customer_segment.value_counts(), explode=explode, labels=rfm_df.Customer_segment.value_counts().index, autopct='%1.1f%%',
        shadow=True, startangle=90,colors=['tab:green','tab:blue','tab:orange','tab:red','tab:purple','tab:brown','tab:pink','tab:gray','tab:olive','tab:cyan'])

ax1.axis('equal')  
plt.tight_layout()
plt.show()

"""# **Question 3**"""

differentiate = pd.read_csv('/content/Data analyst Task.csv')
differentiate['discount_type'] = differentiate['discount_type'].fillna(0)
differentiate['created_at']=pd.to_datetime(differentiate['created_at'])

differentiate=differentiate.sort_values(by=['user_id','created_at'])
differentiate

differentiate=differentiate[differentiate.duplicated(subset=['user_id'], keep=False)]
differentiate=differentiate.sort_values(['user_id','created_at'])
#differentiate['diff_line_race'] = differentiate['created_at'] - differentiate['created_at'].shift(1)
differentiate['NextOrder'] = differentiate.groupby(['user_id']).shift(-1)['created_at']
differentiate['difference']=differentiate['NextOrder'].subtract(differentiate['created_at'])
differentiate.head(13)

differentiate['difference'].mean()

differentiate.describe()

#data preprocessing 
differentiate['difference']=differentiate['difference'].dt.days
differentiate['difference'] = differentiate['difference'].fillna(0)

differentiate=differentiate.loc[differentiate['difference'] != 0]
differentiate

sns.pairplot(differentiate[['user_id','difference','basket','discount_cost','discount_type']])

"""# **Implement a Kmeans clustering algorithm**"""

import sklearn.cluster as cluster
# use 3 cluster
kmeans = cluster.KMeans(n_clusters=3 ,init="k-means++")
kmeans = kmeans.fit(differentiate[['difference','basket']])
differentiate['Clusters'] = kmeans.labels_


sns.scatterplot(x="difference", y="basket",hue = 'Clusters',  data=differentiate)

sns.pairplot(differentiate[['user_id','difference']])